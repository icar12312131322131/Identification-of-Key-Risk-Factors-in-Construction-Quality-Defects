import pandas as pd
import jieba
import jieba.posseg as pseg
from gensim import corpora, models
import matplotlib.pyplot as plt
import re
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, models as st_models
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestNeighbors
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.manifold import TSNE
import openpyxl
import multiprocessing
from warnings import filterwarnings
from typing import List, Set, Tuple, Any, Dict
from collections import defaultdict

filterwarnings('ignore')

# -------------------
# å¢å¼ºé…ç½®å‚æ•°
class Config:
    DATA_PATH = r"D:\ä¸œè¥¿ä¸­ä¸œåŒ—\ä¸œéƒ¨_combined_by_period.xlsx"          # æ•°æ®è·¯å¾„
    SHEET_NAME = "Sheet1"                  # å·¥ä½œè¡¨åç§°
    TEXT_COLUMN = "sentence"                     # æ–‡æœ¬åˆ—ï¼ˆAåˆ—ï¼‰
    STOPWORDS_PATH = r"D:\æ–‡æœ¬æŒ–æ˜\hit_stopwords.txt"   # HITåœç”¨è¯
    DOMAIN_WORDS_PATH = r"D:\æ–‡æœ¬æŒ–æ˜\äººæ–‡æ”¿æ²».txt"  # é¢†åŸŸä¸“ç”¨è¯    
    MIN_TOPICS = 5                       # æœ€å°ä¸»é¢˜æ•°
    MAX_TOPICS = 5                       # æœ€å¤§ä¸»é¢˜æ•°
    PASSES = 500                          # LDAè®­ç»ƒè½®æ¬¡
    RANDOM_SEED = 42                      # éšæœºç§å­
    AUTOENCODER_EPOCHS = 100              # è‡ªåŠ¨ç¼–ç å™¨è®­ç»ƒè½®æ¬¡
    BATCH_SIZE = 64                       # æ‰¹é‡å¤§å°
    ENCODING_DIM = 64                     # æ½œåœ¨ç©ºé—´ç»´åº¦
    CLUSTER_PLOT_SAMPLE = 3800            # èšç±»å¯è§†åŒ–é‡‡æ ·æ•°
    NUM_WORKERS = max(1, multiprocessing.cpu_count() // 2)  # å¹¶è¡Œè¿›ç¨‹æ•°
    EMBEDDING_MODEL = "moka-ai/m3e-large"  # åµŒå…¥æ¨¡å‹
    CANOPY_SAMPLE_SIZE = 5000             # å¯†åº¦Canopyé‡‡æ ·å¤§å°

cfg = Config()

# -------------------
# å·¥å…·å‡½æ•°
def load_stopwords(path: str) -> Set[str]:
    """åŠ è½½åœç”¨è¯ï¼Œæ”¯æŒå¤šç§ç¼–ç æ ¼å¼"""
    encodings = ['utf-8', 'gbk', 'latin1']
    for enc in encodings:
        try:
            with open(path, 'r', encoding=enc) as f:
                return {line.strip() for line in f if line.strip()}
        except UnicodeDecodeError:
            continue
    raise ValueError(f"æ— æ³•è§£ç æ–‡ä»¶: {path}")

def load_domain_words(path: str) -> Set[str]:
    """åŠ è½½é¢†åŸŸè¯å…¸ï¼Œå¹¶æ·»åŠ åˆ°jiebaä¸­ä»¥æé«˜åˆ‡åˆ†å‡†ç¡®ç‡"""
    words = load_stopwords(path)
    for word in words:
        jieba.add_word(word, freq=10000)
    return words

def preprocess(text: str, stopwords: Set[str], domain_words: Set[str]) -> List[str]:
    """æ–‡æœ¬é¢„å¤„ç†ï¼šæ¸…æ´—ã€åˆ†è¯ã€è¯æ€§æ ‡æ³¨"""
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", " ", text)
    text = re.sub(r"\s{2,}", " ", text).strip().lower()
    
    words = []
    for word, flag in pseg.cut(text):
        keep_cond = (
            (word in domain_words) or
            (len(word) > 1 and word not in stopwords and flag not in ['x', 'w', 'uj'])
        )
        if keep_cond:
            words.append(word)
    return words

def compute_metrics(args: Tuple[List[Any], corpora.Dictionary, List[List[str]], int]) -> Tuple[int, float, float, Any]:
    """è®¡ç®—LDAæ¨¡å‹æŒ‡æ ‡"""
    corpus, dictionary, texts, num_topics = args
    try:
        lda = models.LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            passes=cfg.PASSES,
            random_state=cfg.RANDOM_SEED,
            alpha='auto'
        )
        perplexity = -lda.log_perplexity(corpus)
        coherence = models.CoherenceModel(
            model=lda, texts=texts, dictionary=dictionary, 
            coherence='c_v', processes=1
        ).get_coherence()
        return num_topics, perplexity, coherence, lda
    except Exception as e:
        print(f"ä¸»é¢˜æ•° {num_topics} å‡ºé”™: {str(e)}")
        return num_topics, None, None, None

def build_sbert_model() -> SentenceTransformer:
    """æ„å»ºSBERTåµŒå…¥æ¨¡å‹"""
    try:
        word_embedding = st_models.Transformer(
            cfg.EMBEDDING_MODEL, 
            model_args={"trust_remote_code": True}
        )
        pooling = st_models.Pooling(
            word_embedding.get_word_embedding_dimension(),
            pooling_mode='mean'
        )
        return SentenceTransformer(modules=[word_embedding, pooling])
    except Exception as e:
        raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

def parallel_lda_processing(texts: List[List[str]]) -> Tuple[corpora.Dictionary, List[List[Tuple[int, int]]]]:
    """æ„å»ºè¯å…¸å’Œè¯­æ–™"""
    dictionary = corpora.Dictionary(texts)
    dictionary.filter_extremes(
        no_below=max(5, int(len(texts) * 0.001)),
        no_above=0.5,
        keep_n=100000
    )
    corpus = [dictionary.doc2bow(doc) for doc in texts]
    return dictionary, corpus

def enhanced_autoencoder(input_dim: int) -> Tuple[Model, Model]:
    """æ„å»ºè‡ªåŠ¨ç¼–ç å™¨"""
    input_layer = Input(shape=(input_dim,))
    x = Dense(512, activation='swish')(input_layer)
    x = Dropout(0.3)(x)
    x = Dense(256, activation='swish')(x)
    x = Dropout(0.2)(x)
    encoded = Dense(cfg.ENCODING_DIM, activation='swish')(x)
    
    x = Dense(256, activation='swish')(encoded)
    x = Dropout(0.2)(x)
    x = Dense(512, activation='swish')(x)
    x = Dropout(0.3)(x)
    decoded = Dense(input_dim, activation='linear')(x)
    
    autoencoder = Model(input_layer, decoded)
    autoencoder.compile(optimizer='adam', loss='mse')
    return autoencoder, Model(input_layer, encoded)

def density_canopy_kmeans(X: np.ndarray, random_state: int = None) -> Tuple[np.ndarray, int]:
    """æ”¹è¿›çš„å¯†åº¦Canopyç®—æ³•"""
    if len(X) > cfg.CANOPY_SAMPLE_SIZE:
        sample_indices = np.random.choice(len(X), cfg.CANOPY_SAMPLE_SIZE, replace=False)
        X_sample = X[sample_indices]
    else:
        X_sample = X

    nbrs = NearestNeighbors(n_neighbors=3800, algorithm='kd_tree').fit(X_sample)
    # åŒæ—¶è¿”å›è·ç¦»å’Œå¯¹åº”çš„ç´¢å¼•
    distances, indices_knn = nbrs.kneighbors(X_sample)
    mean_dis = np.mean(distances[:, 1:])*1.32  # å¿½ç•¥è‡ªèº«ï¼ˆç¬¬ä¸€ä¸ªè·ç¦»ä¸º0ï¼‰

    rho = np.sum(distances < mean_dis, axis=1)
    a = np.zeros(len(X_sample))
    for i in range(len(X_sample)):
        # ä½¿ç”¨è¿”å›çš„ç´¢å¼•æ•°ç»„ï¼Œä¿è¯æ¯”è¾ƒçš„æ•°ç»„å½¢çŠ¶ä¸€è‡´
        neighbors = np.where((distances[i] < mean_dis) & (indices_knn[i] != i))[0]
        if len(neighbors) > 1:
            sub_dist = distances[np.ix_(neighbors, neighbors)]
            a[i] = np.sum(sub_dist) / (len(neighbors) * (len(neighbors) - 1))
    
    # ä½¿ç”¨å…¨è·ç¦»çŸ©é˜µæ¥è®¡ç®—deltaï¼Œé¿å…è¶…å‡ºé¢„è®¡ç®—é‚»å±…è·ç¦»çš„èŒƒå›´
    full_dists = np.linalg.norm(X_sample[:, None] - X_sample[None, :], axis=2)
    delta = np.zeros(len(X_sample))
    for i in range(len(X_sample)):
        higher_rho = np.where(rho > rho[i])[0]
        if len(higher_rho) > 0:
            delta[i] = np.min(full_dists[i, higher_rho])
        else:
            delta[i] = np.max(full_dists[i, :])
    
    epsilon = 1e-6
    w = rho * (1.0 / (a + epsilon)) * delta

    remaining = np.arange(len(X_sample))
    centers = []
    while len(remaining) > 0:
        idx = remaining[np.argmax(w[remaining])]
        centers.append(X_sample[idx])
        # è®¡ç®—é€‰ä¸­ä¸­å¿ƒä¸æ‰€æœ‰æ ·æœ¬ä¹‹é—´çš„æ¬§å¼è·ç¦»
        center_dists = np.linalg.norm(X_sample - X_sample[idx], axis=1)
        mask = center_dists[remaining] <= mean_dis
        remaining = remaining[~mask]

    k = len(centers) if centers else 1
    if not centers:
        centers = [X_sample[np.argmax(rho)]]
    
    # ä½¿ç”¨å®Œæ•´æ•°æ®è¿›è¡ŒK-meansèšç±»
    kmeans = KMeans(n_clusters=k, init=np.array(centers), n_init=1, random_state=random_state)
    full_labels = kmeans.fit_predict(X)
    return full_labels, k

def lda_vector_generator(corpus: List[List[Tuple[int, int]]], lda_model: Any) -> np.ndarray:
    """ç”Ÿæˆç»Ÿä¸€ç»´åº¦çš„LDAå‘é‡"""
    num_topics = lda_model.num_topics
    vectors = []
    for doc in corpus:
        topic_probs = dict(lda_model.get_document_topics(doc, minimum_probability=0))
        vec = [topic_probs.get(i, 0.0) for i in range(num_topics)]
        vectors.append(vec)
    return np.array(vectors)

def visualize_results(history: Any, latent: np.ndarray, labels: np.ndarray,
                     topics: List[int], perplexities: List[float], coherences: List[float],
                     best_topic: int) -> None:
    """å¯è§†åŒ–ç»“æœ"""
    plt.figure(figsize=(20, 7), dpi=190)
    
    # LDAæŒ‡æ ‡
    ax1 = plt.subplot(1, 3, 1)
    ax1.plot(topics, perplexities, 'b-o', markersize=8)
    ax1.set_xlabel("ä¸»é¢˜æ•°é‡", fontsize=12)
    ax2 = ax1.twinx()
    ax2.plot(topics, coherences, 'r-*', markersize=10)
    ax2.set_ylabel("ä¸€è‡´æ€§", color='r', fontsize=12)
    plt.title("LDAæ¨¡å‹æŒ‡æ ‡è¶‹åŠ¿", fontsize=14)
    
    # è‡ªç¼–ç å™¨æŸå¤±
    plt.subplot(1, 3, 2)
    plt.semilogy(history.history['loss'], label='è®­ç»ƒé›†')
    plt.semilogy(history.history['val_loss'], label='éªŒè¯é›†')
    plt.title("è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ æ›²çº¿", fontsize=14)
    
    # èšç±»å¯è§†åŒ–
    plt.subplot(1, 3, 3)
    sample_size = min(cfg.CLUSTER_PLOT_SAMPLE, len(latent))
    sample_idx = np.random.choice(len(latent), sample_size, replace=False)
    tsne = TSNE(n_components=2, perplexity=30, random_state=cfg.RANDOM_SEED)
    vis_data = tsne.fit_transform(latent[sample_idx])
    
    sc = plt.scatter(vis_data[:, 0], vis_data[:, 1],
                     c=labels[sample_idx], cmap='Spectral', alpha=0.7)
    cbar = plt.colorbar(sc, boundaries=np.arange(np.max(labels)+2)-0.5)
    cbar.set_ticks(np.arange(np.max(labels)+1))
    plt.title("èšç±»åˆ†å¸ƒå¯è§†åŒ–", fontsize=14)
    
    plt.tight_layout()
    plt.savefig("analysis_results.jpg", bbox_inches='tight')
    plt.close()

def save_results(raw_texts: List[str], processed_texts: List[List[str]], labels: np.ndarray,
                 lda_model: Any, corpus: List[List[Tuple[int, int]]]) -> None:
    """ä¿å­˜ç»“æœ"""
    # ç”Ÿæˆèšç±»å…³é”®è¯
    cluster_words = defaultdict(list)
    for label, words in zip(labels, processed_texts):
        cluster_words[label].extend(words)
    
    topic_keywords = []
    for cluster in sorted(cluster_words.keys()):
        word_freq = defaultdict(int)
        for word in cluster_words[cluster]:
            word_freq[word] += 1
        top_words = [w for w, _ in sorted(word_freq.items(), key=lambda x: (-x[1], x[0]))[:90]]
        topic_keywords.append(", ".join(top_words))
    
    # ç”ŸæˆDataFrame
    result_df = pd.DataFrame({
        "åŸå§‹æ–‡æœ¬": raw_texts[:len(processed_texts)],
        "åˆ†è¯ç»“æœ": [" ".join(t) for t in processed_texts],
        "èšç±»æ ‡ç­¾": labels,
        "ä¸»é¢˜å…³é”®è¯": [topic_keywords[label] for label in labels]
    })
    
    summary_df = pd.DataFrame({
        "èšç±»æ ‡ç­¾": range(len(topic_keywords)),
        "æ ·æœ¬æ•°é‡": result_df.groupby("èšç±»æ ‡ç­¾").size(),
        "ä»£è¡¨æ€§å…³é”®è¯": topic_keywords
    })
    
    with pd.ExcelWriter("èšç±»åˆ†æç»“æœ.xlsx") as writer:
        result_df.to_excel(writer, sheet_name="è¯¦ç»†ç»“æœ", index=False)
        summary_df.to_excel(writer, sheet_name="èšç±»æ‘˜è¦", index=False)
    
    print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°èšç±»åˆ†æç»“æœ.xlsx")

# -------------------
# ä¸»æµç¨‹
def main() -> None:
    # 1. æ•°æ®åŠ è½½
    print("ğŸ”„ [1/8] åŠ è½½æ•°æ®...")
    try:
        df = pd.read_excel(cfg.DATA_PATH, sheet_name=cfg.SHEET_NAME,
                           usecols=[cfg.TEXT_COLUMN], engine='openpyxl')
        texts_raw = df.iloc[:, 0].astype(str).str.strip().tolist()
        print(f"ğŸ“Š åˆå§‹æ–‡æœ¬æ•°: {len(texts_raw)}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½é”™è¯¯: {repr(e)}")
        return

    # 2. åŠ è½½è¯­è¨€èµ„æº
    print("ğŸ”„ [2/8] åŠ è½½è¯­è¨€èµ„æº...")
    try:
        stopwords = load_stopwords(cfg.STOPWORDS_PATH)
        domain_words = load_domain_words(cfg.DOMAIN_WORDS_PATH)
    except Exception as e:
        print(f"âŒ èµ„æºåŠ è½½é”™è¯¯: {str(e)}")
        return

    # 3. é¢„å¤„ç†
    print("ğŸ”„ [3/8] å¹¶è¡Œé¢„å¤„ç†æ–‡æœ¬...")
    valid_indices = []  # è®°å½•æœ‰æ•ˆæ–‡æœ¬çš„ç´¢å¼•
    processed_texts = []
    with multiprocessing.Pool(cfg.NUM_WORKERS) as pool:
        tasks = [(t, stopwords, domain_words) for t in texts_raw]
        for idx, result in enumerate(tqdm(pool.starmap(preprocess, tasks), desc="é¢„å¤„ç†è¿›åº¦")):
            if result and len(result) >= 1:  # è‡³å°‘ä¿ç•™ä¸€ä¸ªæœ‰æ•ˆè¯
                processed_texts.append(result)
                valid_indices.append(idx)
    
    # ä»…ä¿ç•™æœ‰æ•ˆåŸå§‹æ–‡æœ¬
    texts_raw = [texts_raw[i] for i in valid_indices]
    print(f"âœ… æœ‰æ•ˆæ–‡æœ¬: {len(processed_texts)}")

    # 4. LDAè®­ç»ƒ
    print("ğŸ”„ [4/8] è®­ç»ƒLDAæ¨¡å‹...")
    dictionary, corpus = parallel_lda_processing(processed_texts)
    
    with multiprocessing.Pool(cfg.NUM_WORKERS) as pool:
        tasks = [(corpus, dictionary, processed_texts, n)
                 for n in range(cfg.MIN_TOPICS, cfg.MAX_TOPICS + 1)]
        results = []
        for res in tqdm(pool.imap(compute_metrics, tasks), total=len(tasks), desc="LDAè®¡ç®—"):
            if res[2] is not None:
                results.append(res)
    
    if not results:
        print("âŒ æ— æœ‰æ•ˆLDAæ¨¡å‹")
        return
    topics, perplexities, coherences, lda_models = zip(*sorted(results, key=lambda x: x[0]))
    best_idx = np.nanargmax(coherences)
    best_lda = lda_models[best_idx]
    best_topic = topics[best_idx]
    print(f"ğŸ¯ æœ€ä¼˜ä¸»é¢˜æ•°: {best_topic} (ä¸€è‡´æ€§: {coherences[best_idx]:.3f})")

    # 5. SBERTåµŒå…¥
    print("ğŸ”„ [5/8] ç”ŸæˆåµŒå…¥å‘é‡...")
    try:
        sbert_model = build_sbert_model()
        sentences = [" ".join(t) for t in processed_texts]
        sbert_embeddings = sbert_model.encode(
            sentences,
            batch_size=cfg.BATCH_SIZE,
            show_progress_bar=True,
            convert_to_numpy=True
        )
    except Exception as e:
        print(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥: {str(e)}")
        return

    # 6. ç‰¹å¾èåˆ
    print("ğŸ”„ [6/8] ç‰¹å¾èåˆ...")
    lda_embeddings = lda_vector_generator(corpus, best_lda)
    assert sbert_embeddings.shape[0] == lda_embeddings.shape[0], \
        f"æ ·æœ¬æ•°ä¸åŒ¹é…: SBERT={sbert_embeddings.shape[0]}, LDA={lda_embeddings.shape[0]}"
    combined = np.hstack((sbert_embeddings, lda_embeddings))
    print(f"SBERTåµŒå…¥å½¢çŠ¶: {sbert_embeddings.shape}")
    print(f"LDAå‘é‡å½¢çŠ¶: {lda_embeddings.shape}")
    print(f"åˆå¹¶åå½¢çŠ¶: {combined.shape}")
    scaler = MinMaxScaler()
    combined_scaled = scaler.fit_transform(combined)

    # 7. è‡ªç¼–ç å™¨
    print("ğŸ”„ [7/8] è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨...")
    autoencoder, encoder = enhanced_autoencoder(combined_scaled.shape[1])
    callbacks = [
        EarlyStopping(patience=15, restore_best_weights=True),
        ReduceLROnPlateau(factor=0.5, patience=5)
    ]
    history = autoencoder.fit(
        combined_scaled, combined_scaled,
        epochs=cfg.AUTOENCODER_EPOCHS,
        batch_size=cfg.BATCH_SIZE,
        validation_split=0.2,
        callbacks=callbacks,
        verbose=0
    )
    latent = encoder.predict(combined_scaled, verbose=0)

    # 8. èšç±»åˆ†æ
    print("ğŸ”„ [8/8] è¿›è¡Œèšç±»åˆ†æ...")
    labels, k = density_canopy_kmeans(latent, random_state=cfg.RANDOM_SEED)
   
    # å¯è§†åŒ–ç»“æœ
    visualize_results(history, latent, labels, list(topics), list(perplexities), list(coherences), best_topic)

    # ä¿å­˜æœ€ç»ˆç»“æœ
    save_results(texts_raw, processed_texts, labels, best_lda, corpus)

if __name__ == "__main__":
    main()
